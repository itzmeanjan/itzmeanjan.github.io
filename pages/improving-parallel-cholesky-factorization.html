<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Improving Parallel Cholesky Factorization
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Improving Parallel Cholesky Factorization">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="Step-by-step Parallel Cholesky Factorization improvements, gaining ~22% performance boost !">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Improving Parallel Cholesky Factorization">
    <meta property="twitter:description" content="Step-by-step Parallel Cholesky Factorization improvements, gaining ~22% performance boost !">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="Step-by-step Parallel Cholesky Factorization improvements, gaining ~22% performance boost !">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords"
        content="dpc++, dpcpp, sycl, oneapi, gpgpu, fpga, accelerator, improving, parallel, cholesky, factorization, simt, cpp">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="https://gist.github.com/itzmeanjan" target="_blank"><big>G</big>ists</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/talks.html"><big>T</big>alks</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Improving Parallel Cholesky Factorization
                </h1>
                <h3>Created : November 01, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    Last week I was working on implementing parallel cholesky factorization on accelerators, using SYCL DPC++, though obtained
                    performance was not impressive due to some points I recognized and noted <a class="blogLink" target="_blank" href="../pages/parallel-cholesky-factorization.html">here</a>.
                    Today I plan to expand on those points, while splitting factorization into computationally ordered and dependent steps, which eventually proves to
                    attain better speed up than previous implementation of mine.
                    <br>
                    <br>
                    I abstractly start working with one symmetric positive definite square matrix A<sub>N x N</sub>, which I convert into U<sub>N x N</sub>. Obtaining the other
                    half is as easy as performing a transpose of U<sub>N x N</sub>, giving me L<sub>N x N</sub>.
                    Once both of these halves are computed, ðŸ‘‡ can be checked, while keeping it in mind that as these computations are performed on accelerated
                    environments, some floating point errors are inevitable.
                </p>
                <img class="imgCenterUpdt" src="../images/improving-parallel-cholesky-factorization-0.png">
                <p class="blogText">
                    I begin with an easy step, where I first initialise U<sub>N x N</sub> with A<sub>N x N</sub> and then parallelly zero out lower triangular portion ( read cells below pivots ) of U<sub>N x N</sub>. During the further course
                    of computation I'll never need to access those memory locations. As lower triangular portion of matrix is not rectangular shaped, I have to
                    spawn <span class="highlight"><i>N x N</i></span> parallel work-items to zero out cells below pivots, though a substantial number of work-items will do nothing useful,
                    which I ensure using conditional statement --- sadly resulting into divergent control flow.
                    <br>
                    <br>
                    I write the respective kernel as ðŸ‘‡
                </p>
                <div class="microlight">
    // see https://github.com/itzmeanjan/matrix_factorization/blob/master/cholesky_factorization.cpp#L15

    19| h.parallel_for<class kernelZeroLower>(
    20|    nd_range<2>{range<2>{dim, dim}, range<2>{1, wg_size}},
        [=](nd_item<2> it) {
            const uint i = it.get_global_id(0);
            const uint j = it.get_global_id(1);

            if (i > j) {
            a_mat_out[i][j] = 0.f;
    27|        }
    28| });
                </div>
                <p class="blogText">
                    Next I've to run through a set of steps for N-many rounds, as matrix dimension is N x N. During each round, I go through following 2 steps of computations, in-order, as they have
                    computational dependency.
                </p>
                <ol>
                    <li>Pivot Calculation</li>
                    <li>Pivot Row Calculation</li>
                </ol>
                <p class="blogText">
                    Actually during each of these N-rounds, a pivot is selected and all operations are performed around it. These operations only compute/ access cells above (or including)
                    pivots i.e. upper triangular portion of matrix.
                    <br>
                    <br>
                    In first step of round-k, say pivot U<sub>k, k</sub> is selected, all I do is atomically compute updated pivot value by subtracting squared values at cells U<sub>i, k</sub>
                    where 0 <= i < k. Primarily I spawned <span class="highlight">N x N</span> work-items for this step and was using conditional statements 
                    so that only work-items representing cells above selected pivot are able to atomically update pivot, by subtracting squared value of self. But this
                    results into divergent control flow, costing high, as most of work-items do nothing useful after work-groups are fetched for computation. But one thing I noticed,
                    during this step only cells above/ including selected pivot are accessed, which opens up opportunity for only spawning <span class="highlight">k</span>
                    work-items, parallelly (and atomically) computing updated value of selected pivot U<sub>k, k</sub>.
                    In following diagram, I depict this step of round-k (= 2), by only spawning <span class="highlight">k(= 2)</span> work-items, atomically computing selected pivot U<sub>2, 2</sub>,
                    by subtracting U<sub>i, 2</sub><sup>2</sup>, where i = {0, 1}.
                </p>
                <img class="imgCenterUpdt" src="../images/improving-parallel-cholesky-factorization-1.jpg">
                <p class="blogText">
                    Before step one is completed, I've to spawn another kernel, for computing a single cell of U<sub>N x N</sub>, by performing square root of selected pivot
                    U<sub>k, k</sub>. It's easy and cheap to spawn a kernel, which accesses single pivot cell and computes updated value, rather than implicitly transferring matrix
                    data to host; computing same and finally again implicitly transferring data back to device. Data movement is expensive, costs lots of computation cycles and I aim to minimize so,
                    which is why for improving performance I decide to spawn a single
                    task kernel, which just updates selected pivot on device, while fully avoiding data movement --- low cost !
                </p>
                <p class="blogText">
                    In second step, I compute updated values of cells, living on selected pivot row ( i.e. A<sub>k, _</sub> ), to be more specific
                    only those rightwards from pivot cell A<sub>k, k</sub>. During this step, I need to access cells from a submatrix of dimension 
                    <span class="highlight">(k + 1) x (N - k)</span>. Following same logic as mentioned before, I decide to only spawn <span class="highlight">N - (k + 1)</span>
                    work-items, which removes need for using conditional statements, required when spawning <span class="highlight">N x N</span> work-items, resulting into
                    most work-items sitting idle. Notice, for any selected <span class="highlight">k ( >= 0 && < N )</span>, only <span class="highlight">N - (k + 1)</span> columns have column index
                    higher than selected pivot on same row --- so this time no divergence in control flow, all work-items do something useful.
                    <br>
                    <br>
                    In following depiction, I use k = 1 i.e. in round-1 of cholesky factorization, I compute N - (k + 1) = 2 cells on pivot row i.e. U<sub>1, i</sub> where i = {2, 3}, while
                    accessing cells from marked submatrix. You may want to take a look at what exactly is computed by each of <span class="highlight">N - (k+1)</span> work-items
                    <a class="blogLink" target="_blank" href="https://github.com/itzmeanjan/matrix_factorization/blob/51b46fc2a49342d5be7d3dfd235bcb956c2b670f/cholesky_factorization.cpp#L76-L82">here</a>.
                </p>
                <img class="imgCenterUpdt" src="../images/improving-parallel-cholesky-factorization-2.jpg">
                <p class="blogText">
                    After completion of both steps, in each round, all cells of pivot row U<sub>k, _</sub> hold final value i.e. what it's supposed to be in upper triangular portion after factorization.
                    And after N-many rounds, upper triangular portion is stored in U<sub>N x N</sub>. 
                    <br>
                    <br>
                    I decide to run benchmarks on updated cholesky factorization implementation.
                    Previous implementation on same hardware performed worse for N = 1024, completing in ~400ms, which is now improved to ~370ms. You may also notice, as matrix dimension
                    keeps increasing, floating point math error also keeps increasing, which is found when comparing L<sub>N x N</sub> x U<sub>N x N</sub> with A<sub>N x N</sub>. As I compile
                    DPCPP program with <span class="highlight">-ffast-math</span> option, allowing fast, aggressive, lossy floating point computation, I expect deviation of this scale.
                </p>
                <div class="microlight">
    running on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz

    Cholesky Factorization:
    
    dimension			        time			max deviation
    32   x   32			         5 ms			7.62939e-06
    64   x   64			         5 ms			3.05176e-05
    128  x  128			        10 ms			0.00012207
    256  x  256			        20 ms			0.000305176
    512  x  512			        67 ms			 0.0012207
    1024 x 1024			       370 ms			0.00610352
                </div>
                <p class="blogText">
                    Finally I run improved implementation on another CPU, getting to much better performance of ~172ms of completion time, for N = 1024.
                    Previous unoptimised version on same hardware with same launch configurations resulted into completion time of ~221ms --- <span class="highlight">~22% performance boost !</span>
                </p>
                <div class="microlight">
    running on Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz

    Cholesky Factorization:
    
    dimension			        time			max deviation
    32   x   32			         8 ms			1.14441e-05
    64   x   64			         6 ms			3.8147e-05
    128  x  128			        10 ms			9.15527e-05
    256  x  256			        21 ms			0.000305176
    512  x  512			        52 ms			0.000976562
    1024 x 1024			       172 ms			0.00524902                    
                </div>
                <p class="blogText">
                    I keep optimised implementation of cholesky factorization <a class="blogLink" target="_blank" href="https://github.com/itzmeanjan/matrix_factorization">here</a> for future reference.
                    In coming days, I'll take up other problems for solving them on accelerated environments, as I generally do.
                    <br>
                    <br>
                    Have a great time !
                </p>
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
