<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Evaluation of Merklization Design and Performance in SYCL
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Evaluation of Merklization Design and Performance in SYCL">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="---">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Evaluation of Merklization Design and Performance in SYCL">
    <meta property="twitter:description" content="---">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="---">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords" content="---">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Evaluation of Merklization Design and Performance in SYCL
                </h1>
                <h3>Created : December 31, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    Around a fortnight back I wrote about OpenCL accelerated Merkle Tree construction technique, while using
                    Rescue Prime Hash as underlying collision resistant, cryptographic hash function for obtaining immediate parent node from two children
                    nodes ( either leaves/ intermediates ). You can find that post <a class="blogLink" href="./opencl-accelerated-merkle-tree-construction.html" target="_blank">here</a>.
                    <br>
                    <br>
                    During last two weeks, I've made some improvements in Rescue Prime Hash implementation and also ported it to SYCL. If you happen to
                    be specifically interested in those changes/ improvements, I suggest you take a look at following pull requests.
                </p>
                <ul>
                    <li><a class="blogLink" href="https://github.com/itzmeanjan/ff-gpu/pull/13" target="_blank">Porting vectorized OpenCL Rescue Prime to SYCL</a></li>
                    <li><a class="blogLink" href="https://github.com/itzmeanjan/ff-gpu/pull/16" target="_blank">Optimizing SYCL Rescue Prime with reduced vector lanes</a></li>
                </ul>
                <p class="blogText">
                    Along with that I also explored a different way of Merklization, where given problem statement is same as before i.e.
                    when N -many leaf nodes of some Binary Merkle Tree are provided with (N - 1) -many intermediate nodes to be computed. 
                    In this new way of Merklization, I change which work-items are tasked to compute which intermediate nodes and how many
                    kernel dispatch rounds are required for computing all intermediate nodes. But, as usual, that itself comes with its benefits and drawbacks.
                    Today in this post, I plan to study design and performance of two Merklization techniques, that I've devised, while
                    using SYCL for implementation.
                    <br>
                    <br>
                    Same as last post, I'm not going to be explaining how I improved Rescue Prime implementation in SYCL, but I note that
                    a huge portion of Merklization performance boost comes from optimized implementation of Rescue Prime in SYCL. 
                    I'll do a deep study of that in coming weeks, as promised before.
                </p>
                <p class="blogText">
                    Let me begin with simple one, which henceforth I'll call <b>merklization approach 1</b>. Here <span class="highlight">merklize</span>
                    function is provided with N -many leaf nodes, where N = 2<sup>i</sup> ∀ i ∈ {1, 2, ...}. (N - 1) -many intermediate nodes
                    are computed by dispatching compute kernel log<sub>2</sub>N times. Say for example, if I start with N = 32, all intermediates are
                    computed in 5 rounds, as shown below.
                </p>
                <div class="microlight">
    N = 32
    
    leaves        = [1; N] # 32 leaf nodes
    intermediates = [0; N] # empty intermediate node allocation

    ## Dispatch 1:
    intermediates[16..32] = merklize(leaves) # pairs total of 32 nodes into 16 intermediates

    ## Dispatch 2:
    intermediates[8..16] = merklize(intermediates[16..32]) # pairs total of 16 nodes into 8 intermediates

    ## Dispatch 3:
    intermediates[4..8] = merklize(intermediates[8..16]) # pairs total of 8 nodes into 4 intermediates

    ## Dispatch 4:
    intermediates[2..4] = merklize(intermediates[4..8]) # pairs total of 4 nodes into 2 intermediates

    ## Dispatch 5:
    intermediates[1..2] = merklize(intermediates[2..4]) # pairs 2 subtrees into root node

    Total intermediate nodes computed = 16 + 8 + 4 + 2 + 1 = 31 ✅
                </div>
                <p class="blogText">
                    You should also notice, there are 5 rounds and each of them are data dependent on previous dispatch round.
                    But good news is that each of these rounds are easily parallelizable, due to absence of any intra-round 
                    data dependency. For some specific dispatch round if input to <i>merklize</i> has N intermediate/ leaf nodes,
                    each pair of consecutive nodes are read by respective work-items ( N >> 1 in total ) which are merged into single Rescue Prime digest
                    and stored in proper memory allocation. There is scope of global memory access coalescing, as non-strided, contiguous
                    memory addresses are accessed by each work-item. Let us take help of a diagram to better appreciate the scope
                    of parallelism.
                </p>
                <img class="imgCenter" src="../images/evaluate-merklization-design-performance-in-sycl-0.jpg">
                <p class="blogText">
                    In above diagram, N ( = 16 ) -many leaf nodes are used for depicting execution flow. It requires 4 kernel
                    dispatch rounds to compute all intermediates and each of them are data dependent on previous round. Each dispatch
                    round shows how many work-items are required to parallelly compute all computable intermediate nodes, by invoking Rescue <span class="highlight">merge</span> function;
                    which work-item accesses which pair of consequtive nodes ( either leaves/ intermediates; leaves applicable only during round 0 ).
                    Note, very first memory location ( resevered for some Rescue Prime digest ) is never accessed by any of dispatched kernels, because
                    (N - 1) -many intermediate nodes are stored in memory allocated for storing total of N -many intermediate nodes, where N is power of 2.
                </p>
                <p class="blogText">
                    Implementing this in SYCL is pretty easy, using USM allocation allows me to perform pointer arithmetics, which is much more
                    familiar compared to SYCL buffer/ sub-buffer/ accessor. I suggest you take a look
                    <a class="blogLink" href="https://github.com/itzmeanjan/ff-gpu/blob/8e480571b15d747ee5747f1853781317b4e5c9ae/merkle_tree.cpp#L3-L102" target="_blank">here</a>
                    for exploring exact implementation details of <b>approach 1</b>.
                </p>
                <p class="blogText">
                    Notice when N is relatively large, say = 2<sup>24</sup>, total 24 kernel dispatch rounds will be required to compute all intermediate
                    nodes. Dispatching kernel requires host-device interaction, which is not cheap. Also OpenCL/ SYCL doesn't yet
                    allow recording commands into a buffer and then replaying same when required to be executed as I can do in Vulkan Compute; with that during actual kernel execution,
                    host-device interaction can be reduced and more work will be ready for offloading, while suffering less from host latencies due to say context switching.
                    Well, OpenCL has <a class="blogLink" href="https://www.khronos.org/registry/OpenCL/specs/3.0-unified/html/OpenCL_Ext.html#cl_khr_command_buffer" target="_blank">this extension</a>
                    just to experiment with that idea, but today I'm working with SYCL. Motivated by this reason, I decided to devise a way
                    so that kernel dispatch round count can be reduced while Merklizing.
                    <br>
                    <br>
                    I'm going to refer to this new Merklization technique as <b>approach 2</b>. Say we've N = 2<sup>24</sup> -many leaf nodes and we're using work-group
                    size B = 2<sup>8</sup>, then number of required kernel dispatch rounds for constructing Merkle Tree using <b>approach 2</b> will be
                    <tt>log<sub>2</sub>(N >> (2 + log<sub>2</sub>B)) + 1</tt>, which turns out to be 15, much lesser than 24, which would be required using <b>approach 1</b>.
                    Also note, it doesn't matter what is work-group size, required #-of kernel dispatch rounds will always be log<sub>2</sub>N using <b>approach 1</b>.
                    But using <b>approach 2</b>, work-group size can influence total #-of required kernel dispatch rounds to compute whole Merkle Tree.
                    If I increase work-group size B to say 2<sup>9</sup>, then following aforementioned formula, I'll be required to dispatch kernel 14 times.
                    But as per my experience, increasing work-group size to that large value benefits less, which we'll explore soon.
                    Say, finally I decide to live with work-group size B = 2<sup>6</sup>, I'll be required to perform 17 rounds of kernel dispatch, which is still lesser than
                    24 rounds, required in <b>approach 1</b>.
                    <br>
                    <br>
                    I've come up with following formula for computing required #-of kernel dispatch rounds using <b>approach 2</b>.
                    You should notice, an appended <tt>+ 1</tt>, in each of these cases, that's coming from <a class="blogLink" href="https://github.com/itzmeanjan/ff-gpu/blob/8e480571b15d747ee5747f1853781317b4e5c9ae/merkle_tree.cpp#L122-L135" target="_blank">here</a>
                    --- the very first round of kernel dispatch reads all N -many leaves from input buffer and writes (N >> 1) -many intermediate nodes
                    ( living just above leaf nodes ) in designated location of output buffer. After that remaining intermediate nodes are computed
                    in one/ many rounds. Note, each of these kernel dispatch rounds are data dependent on previous one, which is due to
                    inherent hierarchical structure of Merkle Tree.
                </p>
                <div class="microlight">
    # see https://github.com/itzmeanjan/ff-gpu/blob/8e480571b15d747ee5747f1853781317b4e5c9ae/merkle_tree.cpp#L137-L147
    #
    # `+ 1` at end of each case comes due to first kernel dispatch
    # https://github.com/itzmeanjan/ff-gpu/blob/8e480571b15d747ee5747f1853781317b4e5c9ae/merkle_tree.cpp#L122-L135

    N = leaf count
    B = work-group size
    R = number of kernel dispatch rounds

    if N >> 2 == B:
        R = 1 + 1
    elif N >> 3 == B
        R = (log2(N >> (2 + log2(B))) + 1) + 1
    else:
        R = log2(N >> (2 + log2(B))) + 1
                </div>
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
