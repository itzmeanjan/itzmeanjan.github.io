<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Improving Parallel LU Decomposition by ~20x
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Improving Parallel LU Decomposition by ~20x">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="Improving performance of Parallel LU factorization by considering small details, gaining ~20x performance boost">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Improving Parallel LU Decomposition by ~20x">
    <meta property="twitter:description" content="Improving performance of Parallel LU factorization by considering small details, gaining ~20x performance boost">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="Improving performance of Parallel LU factorization by considering small details, gaining ~20x performance boost">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords"
        content="dpc++, dpcpp, sycl, oneapi, gpgpu, fpga, accelerator, improving, parallel, lu, factorization, decomposition, cpp, forward, elimination, subgroup, functions, scope, of, parallelism">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Improving Parallel LU Decomposition by ~20x
                </h1>
                <h3>Created : October 15, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    Early this week I was working on Parallel LU factorization implementation for GPGPU, where I discovered obtained
                    performance was not satisfactory. I also noted main reasons behind it can be
                </p>
                <ol>
                    <li>In each iteration a <b>pivot</b> is chosen to be worked on, decreasing dimension of operable submatrix, which in turn reduces scope of parallelism because lots of code paths are doing nothing !</li>
                    <li>On completion of each <b>pivot</b> processing, U<sub>N x N</sub> is copied to A<sub>N x N</sub>, in preparation for next iteration. When N is large, cost of copying is high !</li>
                </ol>
                <p class="blogText">
                    Yesterday I planned to take a look at optimizing parallel LU factorization implementation, mainly focusing on aforementioned two points. Now I've an improved
                    LU factorization implementation which is <span class="highlight">~20x</span> faster than previous one. If you're interested in previous implementation, I recommend
                    taking a look at <a class="blogLink" target="_blank" href="https://itzmeanjan.in/pages/parallel-lu-decomposition.html">here</a>.
                    <br>
                    <br>
                    I'll step by step improve current implementation.
                </p>
                <p class="blogText">
                    Starting with first point, in each iteration I'm working on a submatrix of size <span class="highlight">(N - (i + 1)) x (N - i)</span>
                    where i = {0, 1, 2 ... N-2}. LU decomposition selects a pivot ( read A<sub>i, i</sub> in i<sup>th</sup> iteration ) to work on and then reduces all cells below it to zero, while doing so it
                    records multiplication factors in L matrix ( read lower triangular one ) and respectively updates cells in U matrix ( read upper triangular one ).
                    Say in iteration i = 0, I'm working on pivot A<sub>0, 0</sub>, requiring me to operate on following submatrices.
                </p>
                <img class="imgCenterUpdt" src="../images/improved-parallel-lu-decomposition-image-0.png">
                <p class="blogText">
                    Similarly in iteration i = 1, when selected pivot is A<sub>1, 1</sub>, operable submatrix dimension for L reduces to (N - 2) x 1 and for U it becomes
                    (N -2) x (N - 1).
                </p>
                <img class="imgCenterUpdt" src="../images/improved-parallel-lu-decomposition-image-1.png">
                <p class="blogText">
                    This way it keeps reducing and finally reaches submatrix size of 1 x 2 when N = 4, in iteration i = 2. Point I want to make here, although dimension of operable
                    submatrix keeps reducing, I enqueue all kernel invocation commands with index space of dimension N x N, requiring me to use conditional statements to determine whether
                    certain work-item needs to be active or can be deactivated because its index is out of range. If index of some work item is (r, c) it can be deactivated
                    if <span class="highlight">!(r > i && c >= i)</span>, as it's representing some cell which doesn't need to be processed for selected pivot A<sub>i, i</sub>.
                    For these cases where cells don't need to be processed, it results into code paths doing nothing useful --- reducing throughput. When N is large, in each
                    iteration ( read as <b>i</b> keeps increasing ) disabled code path scope keeps increasing, as operable submatrix dimension is (N - (i+1)) x (N - i), lots of work items
                    doing nothing, a huge number of work groups are just launched without doing anything helpful for speeding up computation.
                </p>
                <img class="imgCenterUpdt" src="../images/improved-parallel-lu-decomposition-image-2.png">
                <p class="blogText">
                    I solve this problem using offsets & non-uniform workgroup sizes. Offsets help me in launching kernels with index space dimension (N - (i + 1)) x (N - i) and work-items pointing to only operable cells --- exactly
                    what is desired, which stops the need for using conditional statements in kernel. Now no code paths resulting into disabled work-items, all launched work items
                    doing useful work --- better throughput as GPGPU has enough work to do.
                    <br>
                    <br>
                    Kernel operating on lower triangular matrix is launched with following index space dimension, where i = current iteration index.
                </p>
                <div class="microlight">
    h.parallel_for(nd_range<1>{range<1>{N - (i + 1)}, range<1>{B}, id<1>{i + 1}}, [=](nd_item<1> it) {
        ...
    });
                </div>
                <p class="blogText">
                    While upper triangular matrix is processed with following index space & offset settings.
                </p>
                <div class="microlight">
    h.parallel_for(nd_range<2>{range<2>{N - (i + 1), N - i}, range<2>{1, B}, id<2>{i + 1, i}}, [=](nd_item<2> it) {
        ...
    });
                </div>
                <br>
                <p class="blogText">
                    Now I'd like to focus on second point, which can help me in improving performance. At end of each iteration, I'm copying N x N many elements
                    from updated U matrix to A matrix, so that in iteration i+1, I can load from A matrix & compute/ store results in L & U matrices. Note as L matrix
                    just records multiplication factors, no back and forth copying is required. On other hand, U matrix will eventually become upper triangular matrix, at end of iteration i = (N - 2),
                    until that point it's used as intermediate storage so that parallel modification by some work-items doesn't affect computation of others, which were expecting
                    to read unmodified value at some cell (r, c). So at end of each iteration N x N many elements are copied back to A --- which is indeed expensive.
                    <br>
                    <br>
                    A closer look at which cells are computed/ stored in U reveals only <span class="highlight">(N - (i + 1)) x (N - i)</span> many of N x N total cells are required
                    to be copied back to A. In each iteration as <b>i</b> keeps increasing, dimension of copyable submatrix keeps decreasing --- reducing cost. I implement this using
                    sub-buffers, which are pointing to only required cells of U & A. Now in each iteration <span class="highlight">((N - (i + 1)) x (N - i))</span> many elements are copied from U to A,
                    where i = {0, 1, 2 ... , N - 2}.
                </p>
                <div class="microlight">
    ...

    accessor acc_matrix{b_matrix, h, range<2>{N - (i + 1), N - i}, id<2>{i + 1, i}}; // sub-buffer
    accessor acc_udiag{b_udiag, h, range<2>{N - (i + 1), N - i}, id<2>{i + 1, i}}; // spanning over only modified cells of U

    h.copy(acc_udiag, acc_matrix); // copy(src, dst)

    ...
                </div>
                <br>
                <p class="blogText">
                    I also notice in each iteration pivot A<sub>i, i</sub> is loaded multiple times from global memory, which is high latency, so I introduced work group local memory,
                    which is much faster, for reading A<sub>i, i</sub> only once & afterwards all work items in subgroup reading from fast local memory. I select subgroup leader
                    to load A<sub>i, i</sub> from global memory and store in designated address in local memory.
                </p>
                <div class="microlight">
    ...
    
    1| const sycl::ONEAPI::sub_group sg = it.get_sub_group();
    2| 
    3| if (sycl::ONEAPI::leader(sg)) {
    4|    acc_lds[0] = acc_udiag[i][i]; // storing in local memory
    5| }
    6|
    ...
                </div>
                <p class="blogText">
                    This also comes at a cost, requiring me to synchronize all work items in subgroup so that everyone sees same modification performed in <span class="highlight">acc_lds[0]</span>.
                    I use subgroup barrier to explicitly synchronize all work items in subgroup.
                </p>
                <div class="microlight">
    ...
    6|
    7| sg.barrier(); // synchronization point
    8|
    ...
    // all work items reading from `acc_lds[0]`
                </div>
                <br>
                <p class="blogText">
                    At beginning of LU decomposition, L matrix is prepared by initializing to I<sub>N x N</sub>. In previous version of LU factorization implementation,
                    I was launching a 2D kernel for covering all indices in N x N index space, where work items with <span class="highlight">r == c</span> only did something useful, where r, c = indices along Y, X -axis respectively.
                    I see this can be improved by launching a 1D kernel with index space N, where every invocation does useful work by storing 1 in pivot ( read A[i, i], where i âˆˆ [0, N) ).
                </p>
                <p class="blogText">
                    At this point, most expensive part is copying <span class="highlight">(N - (i +1 )) x (N - i)</span> elements from U to A, at end of each iteration. Though with increase of <b>i</b>,
                    submatrix dimension keeps decreasing, it's still not negligible for large N.
                    I found in each iteration, the cells of U matrix which are modified without any computational dependency with others can be encapsulated
                    inside <span class="highlight">(N - (i + 1)) x (N - (i + 1))</span> submatrix, while another submatrix of dimension <span class="highlight">(N - (i + 1)) x 1</span>
                    can be computed after first one is computed, effectively establishing one dependency. These splitted computation brings benefit that each work-item
                    can be computed in parallel without any data dependency. Effectively in U matrix <span class="highlight">(N - (i +1 )) x (N - i)</span> cells are
                    being modified in each iteration, just splitted in two ordered steps.
                </p>
                <img class="imgCenter" src="../images/improved-parallel-lu-decomposition-image-3.jpg">
                <p class="blogText">
                    With all these improvements, I'm ready to compare LU factorization's performance for 1024 x 1024 matrix. First I run naive version
                    with work group size 64.
                </p>
                <div class="microlight">
    $ dpcpp -fsycl lu_decomposition.cpp && ./a.out

    running on Intel Xeon Processor (Skylake, IBRS)
    LU decomposition: 3870 ms
    max deviation: 0.0339518
                </div>
                <p class="blogText">
                    Now running improved LU factorization implementation with same work group size.
                </p>
                <div class="microlight">
    $ dpcpp -fsycl improved_lu_decomposition.cpp && ./a.out

    running on Intel Xeon Processor (Skylake, IBRS)
    LU decomposition: 162 ms
    max deviation: 0.498501
                </div>
                <p class="blogText">
                    <span class="highlight">~23x</span> speed up gained after fine tuning SYCL DPC++ implementation. I keep optimized version of LU factorization <a class="blogLink" target="_blank" href="https://gist.github.com/itzmeanjan/82e57b64420c417d55133bd443aebdf6">here</a> for future reference.
                    In coming days, I plan to work on other parallel algorithms targeting accelerators, while also optimizing them by considering small details.
                    <br>
                    <br>
                    Have a great time !
                </p>
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
