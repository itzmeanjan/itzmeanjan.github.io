<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Optimizing Parallel Similarity Transformation
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Optimizing Parallel Similarity Transformation">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="...">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Optimizing Parallel Similarity Transformation">
    <meta property="twitter:description" content="...">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="...">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords"
        content="dpc++, dpcpp, sycl, oneapi, gpgpu, fpga, accelerator, similarity, transform, method, maximum, eigen value, eigen vector, positive, square, matrix, simt, cpp">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Optimizing Parallel Similarity Transformation
                </h1>
                <h3>Created : November 27, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    Two weeks ago I wrote about <a class="blogLink" target="_blank" href="https://itzmeanjan.in/pages/parallel-similarity-transform-method.html">Parallel Similarity Transformation</a>, 
                    which is useful iterative method for finding maximum eigen value of positive square matrices, 
                    along with respective eigen vector such that following equality is satisfied. 
                    I used SYCL DPC++ for implementing kernels, but in last weeks I discovered ways
                    to improve upon my previous work. So today I decided to write about how I optimize 
                    similarity transformation kernels and present you with updated benchmark results.
                </p>
                <div class="microlight">
    Av = λv

    A = positive square matrix
    λ = (max) eigen value
    v = (respective) eigen vector
                </div>
                <p class="blogText">
                    I'll not go through algorithmic steps this time, rather I'll pick each kernel required for computing maximum eigen value
                    of positive square matrix and figure out ways how it can be improved. Following five kernels are required for performing
                    parallel similarity transformation.
                </p>
                <ol>
                    <li>kernelSumAcrossRows</li>
                    <li>kernelMaxInVector</li>
                    <li>kernelComputeEigenVector</li>
                    <li>kernelSimilarityTransform</li>
                    <li>kernelStopCriteria</li>
                </ol>
                <p class="blogText">
                    I'll start with first one. All this kernel should do is simply compute sum of values stored in cells along each row of matrix. A<sub>N x N</sub>
                    being a square matrix, it needs to output a N-element vector where i<sup>th</sup> cell should hold sum of i<sup>th</sup> row of matrix. I want to
                    perform N-many parallel summations --- as simple as that. With this requirement in mind, I'll write following kernel.
                </p>
                <div class="microlight">
    // @note this is a pseudocode !
    // kernelSumAcrossRows_v0
    //
    // check https://github.com/itzmeanjan/eigen_value/blob/e584beda3b05759673bce5af7ebed95fd262dff2/benchmarks/benchmark_similarity_transform.cpp#L39-L53

    float *mat = ...; // matrix I'm operating on of dimension N x N
    float *vec = ...; // row sum result vector of length N

    N = 1 << 10;
    B = 1 << 5;

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {
        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c); // value stored here

        atomic_add(*(vec + r), cell);
    });
                </div>
                <p class="blogText">
                    A high-level look at this kernel reveals it's not good idea to atomically compute sum of all elements along each row of A<sub>N x N</sub>. 
                    N-many memory locations of result vector of length N, are being operated on atomically to avoid any chance of data race. Each cell of result
                    vector of length N, will be accessed N-many times by N-many designated work-items. This results into N<sup>2</sup> global memory access, which is expensive.
                    I'd like to reduce global memory access as much as possible. One simple way to do this in current context is to make use of reduction function
                    of SYCL DPC++, which will help me in computing sum of all elements held by all work-items of subgroup. Using this subgroup method maps
                    well to underlying SIMD assembly function, which reduces communication cost, but does compute sum of all values held by subgroup participants. 
                    Finally I'll use atomic write to update sum in designated row of result vector, living in global memory, helping me to reduce global memory access
                    to N<sup>2</sup> / B, given that B is subgroup width. I'll express this with following kernel.
                </p>
                <div class="microlight">
    // @note this is a pseudocode
    // kernelSumAcrossRows_v1
    //
    // check https://github.com/itzmeanjan/eigen_value/blob/e584beda3b05759673bce5af7ebed95fd262dff2/benchmarks/benchmark_similarity_transform.cpp#L86-L106

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {
        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c);

        // reduction method called over subgroup work-items
        //
        // should result into better SIMD usage
        float subgroup_sum = reduce_sum(cell);

        // only sub-group leader accesses global memory one time !
        if(leader()) {
            atomic_add(*(vec + r), subgroup_sum);
        }
    });
                </div>
                <p class="blogText">
                    This kernel is indeed better than previous matrix row sum kernel, where N<sup>2</sup> global memory accesses were performed. But I see
                    one more way to attempt further optimization. I want to make use of faster local memory so that subgroup leader can atomically 
                    update respective work-group's sum, stored in designated local memory.
                    It's possible to have multiple subgroups in a work-group, that way I'm able to reduce global memory access to another factor. Now only subgroup leaders
                    are accessing local memory to update respective work-group's row sum, with their own subgroup level row sum. Finally after all work-items of
                    some work-group are done, only work-group leader will access designated cell of global memory to atomically update respective row sum. 
                    With this update kernel should only access global memory N<sup>2</sup> / B times, when B = work-group size, which should preferrably be greater than subgroup size. 
                    My final update to matrix row sum kernel looks like below.
                </p>
                <div class="microlight">
    // @note this is a pseudocode
    // kernelSumAcrossRows_v2
    //
    // check https://github.com/itzmeanjan/eigen_value/blob/d27f64c27a7a38c7b1038a49563ec4ce47e9ac19/similarity_transform.cpp#L90-L135

    float *local_memory = ...; // local-data-share of size 1

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {

        if(leader()) {
            // initialised by work-group leader
            //
            // if not done, it might prove costly 
            *local_memory = 0.f;
        }

        // only after all work-items of work-group 
        // reaches this point, move forward !
        barrier();

        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c);

        // reduction method called over subgroup work-items
        //
        // should result into better SIMD usage
        float subgroup_sum = reduce_sum(cell);

        // subgroup leader should only update
        // subgroup level row sum in work-group
        // local memory
        if (leader()) {
            atomic_add(*local_memory, subgroup_sum);
        }

        // only after all work-items of work-group 
        // reaches this point, move forward !
        barrier();

        // only work-group leader accesses global memory one time !
        if(leader()) {
            atomic_add(*(vec + r), *local_memory);
        }
    });
                </div>
                <p class="blogText">
                    Now it's good time to run these three kernels on both CPU and GPU, with various matrix dimension, to see how they perform. Following section shows
                    result of running three variants of row sum kernel on CPU.
                </p>
                <div class="microlight">
    running on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz

    [kernel] Sum Across Rows of Matrix (v0)

    128  x  128			     2.688 ms
    256  x  256			     0.602 ms
    512  x  512			     1.294 ms
    1024 x 1024			     5.474 ms
    2048 x 2048			    20.816 ms
    4096 x 4096			    79.719 ms
    8192 x 8192			     315.3 ms

    [kernel] Sum Across Rows of Matrix (v1)

    128  x  128			     1.235 ms
    256  x  256			     0.219 ms
    512  x  512			     0.363 ms
    1024 x 1024			      1.04 ms
    2048 x 2048			     3.746 ms
    4096 x 4096			    14.896 ms
    8192 x 8192			     59.13 ms

    [kernel] Sum Across Rows of Matrix (v2)

    128  x  128			      0.23 ms
    256  x  256			     0.431 ms
    512  x  512			     0.683 ms
    1024 x 1024			      2.12 ms
    2048 x 2048			     8.167 ms
    4096 x 4096			    32.721 ms
    8192 x 8192			   126.747 ms
                </div>
                <p class="blogText">
                    I see for random square matrix of dimension N x N, second variant of row summation kernel performs best. I expected third variant to
                    perform at least as same as second one, though it seems second one is ~2x faster compared to last kernel. I compiled these kernels Ahead of Time,
                    so that runtime kernel compilation time can be saved and launch will be faster. As I've access to GPU, I've also ran it there and following are benchmark
                    results I obtained.
                </p>
                <div class="microlight">
    running on Intel(R) Iris(R) Xe MAX Graphics [0x4905]

    [kernel] Sum Across Rows of Matrix (v0)

    128  x  128			     7.844 ms
    256  x  256			    47.783 ms
    512  x  512			   298.784 ms
    1024 x 1024			   1652.19 ms
    2048 x 2048			    7182.5 ms
    4096 x 4096			   35201.8 ms
    8192 x 8192			    191747 ms

    [kernel] Sum Across Rows of Matrix (v1)

    128  x  128			     0.536 ms
    256  x  256			     0.531 ms
    512  x  512			     1.395 ms
    1024 x 1024			     5.726 ms
    2048 x 2048			    22.359 ms
    4096 x 4096			   101.141 ms
    8192 x 8192			   502.316 ms

    [kernel] Sum Across Rows of Matrix (v2)

    128  x  128			     0.617 ms
    256  x  256			     0.624 ms
    512  x  512			     1.378 ms
    1024 x 1024			     3.934 ms
    2048 x 2048			    14.186 ms
    4096 x 4096			    54.654 ms
    8192 x 8192			   220.202 ms
                </div>
                <p class="blogText">
                    Naive implementation i.e. first variant is performing much poor on GPU, but that's what I expected as all work-items are attempting
                    to update global memory atomically, congestion should be high. I see this time third variant shines ! As I understand, in GPU it has
                    dedicated local memory, which provides faster access compared to global memory. That's why third variant, where I make use of local memory
                    to reduce global memory access and only work-group leaders access global memory one time to atomically update total row sum, performs better than others
                    --- ~2x faster than second variant, its closest competitor.
                </p>
           </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
