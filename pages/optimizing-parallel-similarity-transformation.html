<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Optimizing Parallel Similarity Transformation
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Optimizing Parallel Similarity Transformation">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="...">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Optimizing Parallel Similarity Transformation">
    <meta property="twitter:description" content="...">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="...">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords"
        content="dpc++, dpcpp, sycl, oneapi, gpgpu, fpga, accelerator, similarity, transform, method, maximum, eigen value, eigen vector, positive, square, matrix, simt, cpp">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Optimizing Parallel Similarity Transformation
                </h1>
                <h3>Created : November 27, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    Two weeks ago I wrote about <a class="blogLink" target="_blank" href="https://itzmeanjan.in/pages/parallel-similarity-transform-method.html">Parallel Similarity Transformation</a>, 
                    which is useful iterative method for finding maximum eigen value of positive square matrices, 
                    along with respective eigen vector such that following equality is satisfied. 
                    I used SYCL DPC++ for implementing kernels, but in last weeks I discovered ways
                    to improve upon my previous work. So today I decided to write about how I optimize 
                    similarity transformation kernels and present you with updated benchmark results.
                </p>
                <div class="microlight">
    Av = λv

    A = positive square matrix
    λ = (max) eigen value
    v = (respective) eigen vector
                </div>
                <p class="blogText">
                    I'll not go through algorithmic steps this time, rather I'll pick each kernel required for computing maximum eigen value
                    of positive square matrix and figure out ways how it can be improved. Following five kernels are required for performing
                    parallel similarity transformation.
                </p>
                <ol>
                    <li>kernelSumAcrossRows</li>
                    <li>kernelMaxInVector</li>
                    <li>kernelComputeEigenVector</li>
                    <li>kernelSimilarityTransform</li>
                    <li>kernelStopCriteria</li>
                </ol>
                <p class="blogText">
                    I'll start with first one. All this kernel should do is simply compute sum of values stored in cells along each row of matrix. A<sub>N x N</sub>
                    being a square matrix, it needs to output a N-element vector where i<sup>th</sup> cell should hold sum of i<sup>th</sup> row of matrix. I want to
                    perform N-many parallel summations --- as simple as that. With this requirement in mind, I'll write following kernel.
                </p>
                <div class="microlight">
    // @note this is a pseudocode !

    float *mat = ...; // matrix I'm operating on of dimension N x N
    float *vec = ...; // row sum result vector of length N

    N = 1 << 10;
    B = 1 << 5;

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {
        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c); // value stored here

        atomic_add(*(vec + r), cell);
    });
                </div>
                <p class="blogText">
                    A high-level look at this kernel reveals it's not good idea to atomically compute sum of all elements along each row of A<sub>N x N</sub>. 
                    N-many memory locations of result vector of length N, are being operated on atomically to avoid any chance of data race. Each cell of result
                    vector of length N, will be accessed N-many times by N-many designated work-items. This results into N<sup>2</sup> global memory access, which is expensive.
                    I'd like to reduce global memory access as much as possible. One simple way to do this in current context is to make use of local memory
                    so that atomically work-group local row sum can be computed by accessing faster local memory atomically. After all work-items of work-group
                    are done, work-group local sum will be written by work-group leader to global memory with single memory access. With this I should be able
                    to reduce global memory access to #-of work-group(s) = N<sup>2</sup> / B, where B = work-group size. I'll express this with following kernel.
                </p>
                <div class="microlight">
    // @note this is a pseudocode
    
    float *local_memory = ...; // local-data-share of size 1

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {

        if(leader()) {
            // initialised by work-group leader
            //
            // if not done, it might prove costly 
            *local_memory = 0.f;
        }

        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c);

        atomic_add(*local_memory, cell);

        // only after all work-items of work-group 
        // reaches this point, move forward !
        barrier();

        // only work-group leader accesses global memory one time !
        if(leader()) {
            atomic_add(*(vec + r), *local_memory);
        }
    });
                </div>
                <p class="blogText">
                    This kernel is indeed better than previous matrix row sum kernel, where N<sup>2</sup> global memory accesses were performed. But I see
                    one way to optimize it further. I want to make use of SYCL DPC++ reduction function, so that all work-items of a subgroup are able to compute
                    sum of row values it's pointing to using SIMD intrinsics. Then subgroup leader should atomically update work-group sum in designated local memory.
                    It's possible to have multiple subgroups in a work-group, that way I'm able to reduce local memory access to great extent. Now only subgroup leaders
                    are accessing local memory to update respective work-group's row sum, with their own subgroup level row sum. Finally after all work-items of
                    some work-group are done, only work-group leader will access designated cell of global memory to atomically update respective row sum. My final update
                    to matrix row sum kernel looks like below.
                </p>
                <div class="microlight">
    // @note this is a pseudocode

    float *local_memory = ...; // local-data-share of size 1

    q.parallel_for(nd_range<2>{{N, N}, {1, B}}, [=] (nd_item<2> it) {

        if(leader()) {
            // initialised by work-group leader
            //
            // if not done, it might prove costly 
            *local_memory = 0.f;
        }

        const size_t r = it.get_global_id(0);
        const size_t c = it.get_global_id(1);

        float cell = *(mat + r * N + c);

        // reduction method called over subgroup work-items
        //
        // should result into better SIMD usage
        float subgroup_sum = reduce_sum(cell);

        // subgroup leader should only update
        // subgroup level row sum in work-group
        // local memory
        if (leader()) {
            atomic_add(*local_memory, subgroup_sum);
        }

        // only after all work-items of work-group 
        // reaches this point, move forward !
        barrier();

        // only work-group leader accesses global memory one time !
        if(leader()) {
            atomic_add(*(vec + r), *local_memory);
        }
    });
                </div>
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
