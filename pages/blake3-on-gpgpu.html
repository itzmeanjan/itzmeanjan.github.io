<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        BLAKE3 on GPGPU
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="BLAKE3 on GPGPU">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="---">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="BLAKE3 on GPGPU">
    <meta property="twitter:description" content="---">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="---">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords" content="---">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    BLAKE3 on GPGPU
                </h1>
                <h3>Created : January 15, 2022</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
              <p class="blogText">
                Last week I implemented multiple variants of highly parallelizable cryptographic hash function BLAKE3
                using SYCL and today I'd like to present my collective understanding, which I gained while implementing/ benchmarking
                BLAKE3, targeting heterogeneous accelerator platform(s). BLAKE3 cryptographic hash function easily lends itself well
                to data parallel execution environments like SYCL/ OpenCL. Speaking from high level design point of view, it consists of 
                following two steps.
              </p>
              <ol>
                <li>Splitting the whole input byte array into N -many equilength ( 1024 bytes ) chunks, each of which can be independently compressed in parallel</li>
                <li>Finally it requires constructing one Binary Merkle Tree with N -many leaf nodes, produced in last step as result of chunk compression</li>
              </ol>
              <p class="blogText">
                The root of tree ( first 32 bytes ) is desired cryptographic hash of input byte array. Both of these steps are good candidates for data parallelism. Note, step-1 produces
                N -many leaf nodes of Binary Merkle Tree, which are used for finding root of Merkle Tree in step-2 i.e. step-2 is data dependent on step-1. 
                <br>
                <br>
                In this document, I'll be working with input byte array of length M -bytes such that <span class="highlight"><tt>M = N * 1024, where N = 2<sup>i</sup>, i = {1, 2, 3 ...}</tt></span>. That means
                after execution of step-1 of BLAKE3, I should have power of 2 -many leaf nodes ( = N ), which will be used for computing root of fully balanced Binary Merkle Tree.
                This will simplify both explanation & implementation. I'll walk you through following two techniques of implementing BLAKE3.
              </p>
              <ol>
                <li>Each SYCL work-item compressing single chunk independently</li>
                <li>Each SYCL work-item compressing P = {2, 4, 8, 16} chunks parallelly</li>
              </ol>
              <p class="blogText">
                  Let me start with first approach which is simpler.
                  <br>
                  <br>
                  Let us assume, I've 8KB input which I take as byte array ( say <span class="highlight"><tt>const sycl::uchar *</tt></span> ) and split it into 8 equal sized chunks.
                  Now each of these 1024 -bytes wide chunks can be compressed in parallel. For doing so, I'll dispatch 8 work-items, with work-group size W ( <tt><= 8 && 8 % W == 0</tt> ),
                  where each work-item executes <span class=highlight><tt>compress( ... )</tt></span> function, consuming 1024 -bytes message, into hash state.
                  Once all these 8 work-items complete their execution, each of them output 64 -bytes chaining value ( which is actually BLAKE3 hash state matrix ), 
                  from which first 32 -bytes to be taken
                  as output chaining value of that chunk. These output chaining values are used as leaf nodes of Binary Merkle Tree, which I'm about to construct.
                  <br>
                  <br>
                  In final step of computation, I construct a Binary Merkle Tree from N ( = 8 ) output chaining values. 
                  As Binary Merkle Tree is a hierarchical structure, I need to dispatch multiple rounds of kernels, respecting data dependency. 
                  To be more specific, in this case 3 ( <tt>= log<sub>2</sub>(N), where N = 8</tt> ) rounds will be required.
                  In first dispatch round, I'll dispatch 4 work-items, who will read ( total ) 4 consecutive pairs of output chaining values and interpret each pair of chaining values as left and
                  right child of ( to be computed ) parent node, placed right next to each other ( in ltr order ), as depicted below.
              </p>
              <div class="microlight">
    # cv = chaining value
    # p_cv = parent chaining value
            
    cv   = [0, 1, 2, 3, 4, 5, 6, 7]            # merkle tree leaves
    p_cv = [p_cv_0, p_cv_1, p_cv_2, p_cv_3]    # merkle tree intermediates, just above leaves
            
    p_cv_0     p_cv_1        p_cv_2        p_cv_3

    (0, 1)      (2, 3)      (4, 5)      (6, 7)
     /   \        /  \       /   \       /   \
    /     \      /    \     /     \     /     \
    0      1    2      3    4      5    6      7
              </div>
              <p class="blogText">
                  Computing parent node involves compressing a pair of chaining values, while setting some flags denoting that parent chaining value will be output of <tt>compress( ... )</tt>, 
                  where each chaining value is of 32 -bytes,
                  making total of 64 -bytes input to <tt>compress( ... )</tt>. After completion of this dispatch round, we should have 4 parent nodes, who live just above leaf nodes.
                  In next dispatch round, I've to ask for 2 work-items, each will compress two consecutive chaining values ( which were computed during last round )
                  and produce total 2 parent nodes, who live just below root of the tree ( to be computed in next dispatch round ). 
              </p>
              <div class="microlight">
        p_cv_0                    p_cv_1

    ((0, 1), (2, 3))        ((4, 5), (6, 7))
         /       \                /      \
        /         \              /        \
    (0, 1)      (2, 3)      (4, 5)      (6, 7)
     /   \        /  \       /   \       /   \
    /     \      /    \     /     \     /     \
    0      1    2      3    4      5    6      7
              </div>
              <p class="blogText">
                In final round, it suffices to dispatch just a single task which takes
                64 -bytes input ( read two chaining values, which are two immediate children of root of tree, computed during last round ) and produces 32 -bytes
                output chaining value which is root of Merkle Tree. This root is our desired BLAKE3 hash. Also note, before root of tree can be computed, flag denoting
                that output of <tt>compress( ... )</tt> function invocation is root chaining value of BLAKE3 Merkle Tree, need to be set.
                <br>
                <br>
                A pictorial demonstration might be helpful at this moment.
              </p>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_0.jpg">
              <p class="blogText">
                  Empowered with this high level knowledge of algorithmic construction of BLAKE3, it's good time to dive into often mentioned <span class="highlight"><tt>compress( ... )</tt></span>
                  function. Simply speaking compression starts with 32 -bytes input chaining value and 64 -bytes input message words, it consumes whole message into BLAKE3 hash state
                  ( in multiple rounds, while also employing message permutation, using predefined indexing tricks )
                  and produces output of 64 -bytes, which is nothing but hash state after consuming whole input message inside it. First 32 -bytes of output is taken as chaining value
                  which is either used as input to next stage of computation or as final root chaining value i.e. BLAKE3 digest. Let's emphasize on BLAKE3 hash state.
              </p>
              <p class="blogText">
                  BLAKE3 hash state is 64 -bytes wide; as BLAKE3 word size is 32 -bit, hash state can be represented using an array of 16 elements where each element is 32 -bit wide
                  unsigned integer i.e. <tt>sycl::uint</tt>. When compressing 64 -bytes message, BLAKE3 consumes input message in 7 rounds, while at end of each round ( except last one )
                  permutes 64 -bytes message in a predefined way. At end of applying 7 rounds, it takes first 32 -bytes of hash state, which has now consumed permuted variants of 64 -bytes input
                  message, as output chaining value. Each round of BLAKE3 compression consists of bit wise manipulation of 32 -bit wide hash state words.
              </p>
              <div class="microlight">
    # see https://github.com/BLAKE3-team/BLAKE3/blob/da4c792/reference_impl/reference_impl.rs#L42-L52 👇
    
    # rrot( v, n ) => `v` is rotated right by n -bit places

    def g(sycl::uint state[16], size_t a, size_t b, size_t c, size_t d, sycl::uint mx, sycl::uint my):
        state[a] = state[a] + state[b] + mx
        state[d] = rrot(state[d] ^ state[a], 16)
        state[c] = state[c] + state[d]
        state[b] = rrot(state[b] ^ state[c], 12)
        state[a] = state[a] + state[b] + my
        state[d] = rrot(state[d] ^ state[a], 8)
        state[c] = state[c] + state[d]
        state[b] = rrot(state[b] ^ state[c], 7)

    # see https://github.com/BLAKE3-team/BLAKE3/blob/da4c792/reference_impl/reference_impl.rs#L54-L65 👇

    def blake3_round(sycl::uint state[16], sycl::uint msg[16]):
        # consuming columns
        g(state, 0, 4, 8, 12, msg[0], msg[1])
        g(state, 1, 5, 9, 13, msg[2], msg[3])
        g(state, 2, 6, 10, 14, msg[4], msg[5])
        g(state, 3, 7, 11, 15, msg[6], msg[7])

        # consuming diagonals.
        g(state, 0, 5, 10, 15, msg[8], msg[9])
        g(state, 1, 6, 11, 12, msg[10], msg[11])
        g(state, 2, 7, 8, 13, msg[12], msg[13])
        g(state, 3, 4, 9, 14, msg[14], msg[15])
              </div>
              <p class="blogText">
                  Note indices passed as argument to <tt>g( ... )</tt> function from <tt>blake3_round( ... )</tt>, which clearly shows in first four function invocations, it's column-wise mixing
                  eight message words with hash state. And last four <tt>g( ... )</tt> function invocations are diagonally mixing remaining eight message words with hash state. It's possible to
                  reduce four vertical mixing invocations into single function call, where all four columns are mixed parallelly, 
                  if I represent hash state as an array of 4 vectors ( SYCL intrinsic ), where each vector is of type <tt>sycl::uint4</tt>, as shown below. With this new representation of hash state,
                  diagonal mixing also enjoys boost, where all four diagonals of hash state matrix can be mixed parallelly.
              </p>
              <div class="microlight">
    sycl::uint4 state[4] = {
        sycl::uint4{ ... },
        sycl::uint4{ ... },
        sycl::uint4{ ... },
        sycl::uint4{ ... }
    };
              </div>
              <img class="imgCenterUpdt" src="../images/blake3_on_gpgpu_1.png">
              <p class="blogText">
                  With this new representation of hash state column-wise mixing looks like below.
              </p>
              <div class="microlight">
    # see section 5.3 of BLAKE3 specification https://github.com/BLAKE3-team/BLAKE3-specs/blob/ac78a71/blake3.pdf
    # simd style mixing

    def blake3_round(sycl::uint4 state[4], sycl::uint msg[16]):
        sycl::uint4 mx = { msg[0], msg[2], msg[4], msg[6] }
        sycl::uint4 my = { msg[1], msg[3], msg[5], msg[7] }
        
        # column wise mixing
        state[0] = state[0] + state[1] + mx
        state[3] = rrot(state[3] ^ state[0], 16)
        state[2] = state[2] * state[3]
        state[1] = rrot(state[1] ^ state[2], 12)
        state[0] = state[0] + state[1] + my
        state[3] = rrot(state[3] ^ state[0], 8)
        state[2] = state[2] * state[3]
        state[1] = rrot(state[1] ^ state[2], 7)

        # hash state diagonalisation
        #
        # ... to be written ...

        # diagonal mixing
        #
        # ... to be written ...

        # hash state undiagonalisation
        #
        # ... to be written ...
              </div>
              <p class="blogText">
                  But this 4x4 matrix based hash state form comes with its own requirement, where it needs to be diagonalised such that each diagonal of 4x4 matrix is now in same column, before
                  diagonal mixing can be applied. After diagonal mixing, hash state needs to get back to its original form, which calls for undoing the diagonalisation previously performed.
                  Diagonalisation involves rotating lanes of 4 vectors leftwards by row index of respective vector i.e. {0, 1, 2, 3} in 4x4 state matrix. Note, vector lane rotation doesn't rotate
                  each lane content ( <tt>sycl::uint</tt> ), instead it rotates whole vector by N places. I make use of vector swizzle operators provided by SYCL vector intrinsic API for rotating vector.
                  <br>
                  <br>
                  A pictorial depiction looks like below.
              </p>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_2.jpg">
              <p class="blogText">
                Note the color coding, which shows how diagonalisation helps in bringing each of 4 diagonals of 4x4 hash state matrix in same column. This makes applying diagonal mixing
                much easier ( and faster ) on 128 -bit vectors i.e. <tt>sycl::uint4</tt>.
              </p>
              <div class="microlight">
    # hash state diagonalisation

    state[0] = state[0].xyzw() // can be skipped, doesn't make any change
    state[1] = state[1].yzwx()
    state[2] = state[2].zwxy()
    state[3] = state[3].wxyz()
              </div>
              <p class="blogText">
                  Following code snippet can perform diagonal mixing on four 128 -bit vectors i.e. <tt>sycl::uint4[4]</tt>. This will consume last 8 message words of total 64 -bytes
                  input message into hash state.
              </p>
              <div class="microlight">
    def blake3_round(sycl::uint4 state[4], sycl::uint msg[16]):        
        # column wise mixing
        # ... see above ...
        
        # last 8 message words to be consumed
        sycl::uint4 mz = { msg[8], msg[10], msg[12], msg[14] }
        sycl::uint4 mw = { msg[9], msg[11], msg[13], msg[15] }

        # diagonal mixing
        state[0] = state[0] + state[1] + mz
        state[3] = rrot(state[3] ^ state[0], 16)
        state[2] = state[2] * state[3]
        state[1] = rrot(state[1] ^ state[2], 12)
        state[0] = state[0] + state[1] + mw
        state[3] = rrot(state[3] ^ state[0], 8)
        state[2] = state[2] * state[3]
        state[1] = rrot(state[1] ^ state[2], 7)
              </div>
              <p class="blogText">
                  After diagonal mixing diagonalisation will be undone, rotating vector lanes rightwards by row index of respective vector i.e. {0, 1, 2, 3} in 4x4 state matrix. Following code snippet should bring back
                  hash state is desired form, preparing it for next round.
              </p>
              <div class="microlight">
    # hash state undiagonalisation

    state[0] = state[0].xyzw() // can be skipped, doesn't make any change
    state[1] = state[1].wxyz()
    state[2] = state[2].zwxy()
    state[3] = state[3].yzwx()
              </div>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_3.jpg">
              <p class="blogText">
                  During compression of 64 -bytes message input, after application of each round of mixing, sixteen message words ( 16 x 4 = 64 -bytes total message ) are permuted in following manner
                  and permuted output is used as input message words during next round.
              </p>
              <div class="microlight">
    # see https://github.com/BLAKE3-team/BLAKE3/blob/da4c792/reference_impl/reference_impl.rs#L40
    
    const size_t MSG_PERMUTATION[16] = {2, 6, 3, 10, 7, 0, 4, 13,
                                    1, 11, 12, 5, 9, 14, 15, 8}

    # see https://github.com/itzmeanjan/blake3/blob/e7019ed/include/blake3.hpp#L1623-L1635

    def permute(sycl::uint msg[16]):
        # temporary memory allocation to help permute
        sycl::uint perm[16] = [0] * 16
        
        # permute, loop can be fully unrolled, no loop carried dependency
        # pragma unroll 16
        for i in range(16):
            perm[i] = msg[MSG_PERMUTATION[i]]

        # copy back, loop can be fully unrolled, no loop carried dependency
        # pragma unroll 16
        for i in range(16):
            msg[i] = perm[i]
              </div>
              <p class="blogText">
                  Now let us go back to chunk compression, where we had 1024 -bytes input ( as <tt>sycl::uchar *</tt> ); each <tt>compress( ... )</tt> function invocation
                  takes 64 contiguous bytes to mix with 4x4 hash state matrix, that means we've to iterate 16 times for processing whole chunk. Each of these 64 -bytes
                  are called blocks, 16 of them make a chunk. For first block in a chunk, a predefined constant input chaining value is used, but all subsequent 15 blocks use
                  previous block's 32 -bytes output chaining value as its input chaining value. Note, BLAKE3 hash state is 64 -bytes, input chaining value is 32 -bytes, so remaining 32 -bytes
                  of hash state ( i.e. last two rows of 4x4 hash state matrix ) comes from predefined constants & other parameters passed to <tt>compress( ... )</tt>, which includes flags denoting whether this block
                  is first/ last of this chunk or if output chaining value will be parent/ root node of BLAKE3 Merkle Tree, block length, chunk index etc.. After applying <tt>compress( ... )</tt>
                  first 32 -bytes of hash state is taken as output chaining value, to be used as input chaining value of next block in same chunk. Below is a pictorial demonstration for ease of
                  understanding.
              </p>
              <div class="microlight">
    # definition of compress( ... ) function: https://github.com/itzmeanjan/blake3/blob/e7019ed/include/blake3.hpp#L1728-L1780
              </div>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_4.jpg">
              <p class="blogText">
                  Last block of some chunk produces 32 -bytes output chaining value, which is considered to be leaf node of Binary Merkle Tree. After all chunks are compressed & we've
                  N -many output chaining values, interpreted as leaf nodes of Binary Merkle Tree. Usual parallel Binary Merkle Tree construction algorithm can be applied. Note, for
                  merging two consecutive chaining values ( at leaf level ) into single parent chaining value, <tt>compress( ... )</tt> function is invoked with a predefined constant input
                  chaining value, along with two consecutive chaining values being interpreted as 64 -bytes input message. Some input flags are passed to denote that its output chaining value will be a parent node.
                  And when computing root chaining value ( read target digest of input byte array ), two immediate child nodes just below root are compressed
                  into single chaining value, while passing some flags to denote this is root node being computed.
              </p>
              <div class="microlight">
    see how two child nodes are merged into single parent ( chaining value ) 
    # https://github.com/itzmeanjan/blake3/blob/e7019ed/include/blake3.hpp#L1782-L1803
    
    also see how two immediate children nodes of root of BLAKE3 merkle tree are merged into root ( chaining value )
    # https://github.com/itzmeanjan/blake3/blob/e7019ed/include/blake3.hpp#L1805-L1812
              </div>
              <p class="blogText">
                  It's good time to give second approach a go, where each SYCL work-item compresses more than one chunk.
                  <br>
                  <br>
                  Remember in first approach of parallel BLAKE3, I used a 4x4 matrix of 64 -bytes for representing hash state, when compressing each block of total 1024 -bytes wide chunk.
                  But this time, hash state is represented using a 16 x N matrix, where N = {2, 4, 8, 16} and i<sup>th</sup> row of state matrix holds N -many different chunk's hash state
                  value at index <b>i</b>; so N -many different chunk's hash states are represented using N -many columns of 16 x N shared state matrix. That means, with N = 4, sixteen 128 -bit
                  vectors will be used for representing whole hash state of 4 chunks. Programmatically I'd like to represent it using following syntax.
              </p>
              <div class="microlight">
    # representing N = 4, chunk's hash state
    # each column, represents hash state of i^th chunk, where i ∈ [0, N)
    #
    # s_i_j = j^th chunk's hash state at index i, when each chunk's hash state looks like
    # sycl::uint s_j[16] = { s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9, s_10, s_11, s_12, s_13, s_14, s_15 }

    sycl::uint4 state[16] = {
        sycl::uint4{ s_0_0, s_0_1, s_0_2, s_0_3 },
        sycl::uint4{ s_1_0, s_1_1, s_1_2, s_1_3 },
        sycl::uint4{ s_2_0, s_2_1, s_2_2, s_2_3 },
        sycl::uint4{ s_3_0, s_3_1, s_3_2, s_3_3 },
        sycl::uint4{ s_4_0, s_4_1, s_4_2, s_4_3 },
        sycl::uint4{ s_5_0, s_5_1, s_5_2, s_5_3 },
        sycl::uint4{ s_6_0, s_6_1, s_6_2, s_6_3 },
        sycl::uint4{ s_7_0, s_7_1, s_7_2, s_7_3 },
        sycl::uint4{ s_8_0, s_8_1, s_8_2, s_8_3 },
        sycl::uint4{ s_9_0, s_9_1, s_9_2, s_9_3 },
        sycl::uint4{ s_10_0, s_10_1, s_10_2, s_10_3 },
        sycl::uint4{ s_11_0, s_11_1, s_11_2, s_11_3 },
        sycl::uint4{ s_12_0, s_12_1, s_12_2, s_12_3 },
        sycl::uint4{ s_13_0, s_13_1, s_13_2, s_13_3 },
        sycl::uint4{ s_14_0, s_14_1, s_14_2, s_14_3 },
        sycl::uint4{ s_15_0, s_15_1, s_15_2, s_15_3 },
    };
              </div>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_5.jpg">
              <p class="blogText">
                  With N ( = 4 ) chunks being compressed together, each SYCL work-item mixes total 4096 -bytes of input message into hash state, 
                  each 1024 -bytes chunk splitted in 16 blocks, each of width 64 -bytes. There'll be sixteen rounds required for compressing 4096 -bytes. 
                  In each round, i<sup>th</sup> block of all N chunks are compressed together. Note color coding used in following demonstration, where
                  I attempt to show you how message words ( 32 -bit wide ) of each block are chosen to construct 128 -bit vectors ( using <tt>sycl::uint4</tt> )
                  which are using during column-wise and diagonal mixing. I'd also like you to note that, there's no diagonalisation and undiagonalisation steps
                  required in this SIMD style mixing, because each chunk's hash state is actually a 16 word vector, which is a column of 16 x N state matrix.
                  After first block is processed, which consumes 64 -bytes message from each of four chunks ( i.e. first block of each chunk ), output chaining value
                  of four chunks are prepared by taking 8 x N state matrix, where lower 8 x N portion of matrix ( read last 8 rows ) is dropped. This should produce
                  32 -bytes output chaining value for each chunk, which will be used as input chaining values for those respective chunks when processing block<sub>i+1</sub>
                  from all N ( = 4 ) chunks.
                  <br>
                  <br>
                  After all sixteen blocks from all chunks are compressed into hash state, 16 x N state matrix is truncated to 8 x N matrix ( by dropping last 8 rows ), 
                  which holds N -many output chaining values of N -many chunks. These N -many output chaining values are considered as N -many leaf nodes of BLAKE3 Merkle Tree,
                  which will be constructed once all work-items complete compressing N -many chunks each.
                  <br>
                  <br>
                  Binary Merklization algorithm doesn't anyhow change in second approach.
              </p>
              <img class="imgCenter" src="../images/blake3_on_gpgpu_6.jpg">
              <p class="blogText">
                  Note, when N = 2, sixteen 64 -bit wide SIMD registers are used for representing hash state of two chunks, which are compressed in parallel. Similarly, for N = {4, 8, 16}
                  sixteen {128, 256, 512} -bit registers ( respectively ) will be used for representing hash state of N chunks. On modern CPUs which supports <tt>avx512*</tt> instructions
                  512 -bit vectors can help boosting second SIMD approach.
                  <br>
                  <br>
                  For understanding opportunities of using SIMD for parallelizing BLAKE3 on relatively large input byte arrays, I suggest you take a look at BLAKE3
                  <a class="blogLink" href="https://github.com/BLAKE3-team/BLAKE3-specs/blob/ac78a71/blake3.pdf" target="_blank">specification</a>'s
                  section 5.3.
              </p>
              <p class="blogText">
                  As you've now better understanding of aforementioned two approaches for computing BLAKE3 hash, I'd like to present you with benchmark results. In following tables, you'll
                  see I'm taking random input of N -bytes; transferring whole input to accelerator's accompanying memory; invoking BLAKE3 hash function with on-device data pointer; waiting for
                  all computation steps to complete and finally transferring 32 -bytes digest ( which is output chaining value of root node of Binary Merkle Tree in BLAKE3 hash ) to preallocated
                  memory on host.
              </p>
              <table class="centeredTable">
                  <tr>
                      <th colspan="5">
                          BLAKE3 Hash using <tt>approach_1</tt>
                      </th>
                  </tr>
                  <tr>
                      <th>
                          Input Size
                      </th>
                      <th>
                          Accelerator
                      </th>
                      <th>
                          Kernel Execution Time
                      </th>
                      <th>
                          Host -> Device Tx Time
                      </th>
                      <th>
                          Host <- Device Tx Time
                      </th>
                  </tr>
                  <tr>
                      <td rowspan="4">64 MB</td>
                  </tr>
                  <tr>                           
                    <td>Tesla V100-SXM2-16GB</td>
                    <td>844.598250 us</td>
                    <td>6.166145 ms</td>
                    <td>6.973250 us</td>
                  </tr>
                  <tr>
                    <td>Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</td>
                    <td>6.239875 ms</td>
                    <td>9.797500 ms</td>
                    <td>2.525625 us</td>
                  </tr>
                  <tr>                                                  
                    <td>Intel(R) Iris(R) Xe MAX Graphics</td>
                    <td>4.974242 ms</td>
                    <td>17.749401 ms</td>
                    <td>1.319500 us</td>
                  </tr>

                <tr>
                    <td rowspan="4">128 MB</td>
                </tr>
                <tr>
                  <td>Tesla V100-SXM2-16GB</td>
                  <td>1.800964 ms</td>
                  <td>12.269974 ms</td>
                  <td>7.080000 us</td>
                </tr>
                <tr>
                  <td>Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</td>
                  <td>8.187520 ms</td>
                  <td>20.664062 ms</td>
                  <td>1.242000 us</td>
                </tr>
                <tr>
                  <td>Intel(R) Iris(R) Xe MAX Graphics</td>
                  <td>9.812348 ms</td>
                  <td>35.475108 ms</td>
                  <td>1.319500 us</td>
                </tr>
                <tr>
                    <td rowspan="4">256 MB</td>
                </tr>
                <tr>              
                  <td>Tesla V100-SXM2-16GB</td>
                  <td>3.267731 ms</td>
                  <td>24.462952 ms</td>
                  <td>6.805500 us</td>
                </tr>
                <tr>
                  <td>Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</td>
                  <td>8.853032 ms</td>
                  <td>32.455801 ms</td>
                  <td>1.047125 us</td>
                </tr>
                <tr>                         
                  <td>Intel(R) Iris(R) Xe MAX Graphics</td>
                  <td>19.465823 ms</td>
                  <td>70.886068 ms</td>
                  <td>1.293500 us</td>
                </tr>
                <tr>
                    <td rowspan="4">512 MB</td>
                </tr>
                <tr>
                  <td>Tesla V100-SXM2-16GB</td>
                  <td>5.998047 ms</td>
                  <td>48.833740 ms</td>
                  <td>6.713750 us</td>
                </tr>
                <tr>
                  <td>Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</td>
                  <td>14.807205 ms</td>
                  <td>48.242437 ms</td>
                  <td>1.063000 us</td>
                </tr>
                <tr>                         
                  <td>Intel(R) Iris(R) Xe MAX Graphics</td>
                  <td>39.271700 ms</td>
                  <td>141.716997 ms</td>
                  <td>1.313000 us</td>
                </tr>
                <tr>
                    <td rowspan="4">1024 MB</td>
                </tr>
                <tr>
                  <td>Tesla V100-SXM2-16GB</td>
                  <td>11.915527 ms</td>
                  <td>97.573730 ms</td>
                  <td>8.423000 us</td>
                </tr>
                <tr>
                  <td>Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</td>
                  <td>22.864140 ms</td>
                  <td>79.047688 ms</td>
                  <td>1.088500 us</td>
                </tr>
                <tr>                         
                  <td>Intel(R) Iris(R) Xe MAX Graphics</td>
                  <td>77.556440 ms</td>
                  <td>283.341799 ms</td>
                  <td>1.534000 us</td>
                </tr>
              </table>
              <p class="blogText">
                  In above table, you should see three accelerators I targeted for benchmarking BLAKE3 SIMD <tt>approach_1</tt>, where two of them are GPUs 
                  from two different vendors and one is 64 -core CPU from Intel. You'll notice, Nvidia's Tesla V100 GPU performs best on all input sizes.
                  Being a 64 -core CPU, in majority of cases it performs better compared to Intel's Irix Xe Max GPU. Note, when input size is 64 MB, Intel GPU
                  perform's little better than 64 -core CPU. But as input size increases kernel execution time on Intel GPU starts to quickly increase, though
                  on Intel CPU execution time for 64 MB, 128 MB and 256 MB input sizes are pretty close to each other.
                  <br>
                  <br>
                  As output size is constant ( read 32 -bytes ), device to host data transfer cost is not much of interest. But input data size is variable,
                  host to device data transfer cost tells us is it worth transferring large byte array to accelerator and then computing BLAKE3 hash ?
                  <br>
                  <br>
                  Comparing between multiple accelerators ( with same input size ), it shows as input data size increases host to device data transfer cost
                  increases quickly for GPU ( even surpasses input data transfer cost on CPU for same size ), which makes sense because those accelerators
                  are connected to host over PCIe bus. When comparing input data transfer cost of Nvidia's GPU and Intel's CPU, I see until 512 MB input size,
                  cost was lesser for GPU, but at 512 MB input size both of them take around same time. For both GPUs from two different vendors, I see
                  their host to device data transfer cost increases linearly as input size is increased, because both of them are connected to host CPU using PCIe,
                  which doesn't have high bandwidth. Due to these relatively high input data transfer costs, it may not always benefit using this accelerated
                  BLAKE3 implementation, where explicitly data needs to be transferred to accelerator's local memory, and it may end up defeating whole purpose
                  of speeding up. Just to make it more evident, notice for 1 GB input size on Nvidia Telsa V100 GPU, input transfer is ~8x costlier than actual
                  computation of BLAKE3 hash.
                  <br>
                  <br>
                  Lastly I'd like to draw your attention to device to host data transfer cost ( transferring 32 -bytes digest back to host ), where
                  you should notice, on Nvidia's Tesla V100 GPU it's ~(6 - 7)x more expensive to transfer 32 -bytes ( over PCIe ) to host, when compared to
                  Intel's GPU.
              </p>
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
