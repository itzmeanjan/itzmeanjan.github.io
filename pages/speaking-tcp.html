<!DOCTYPE html>
<!--
  Author: Anjan Roy<hello@itzmeanjan.in>
-->
<html>

<head>
    <title>
        Anjan Roy, Software Engineer, India
    </title>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website">
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="Anjan Roy, Software Engineer, India">
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="My Thoughts, Experiments & Experiences">
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://itzmeanjan.in">
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:secure_url"
        content="https://itzmeanjan.in/images/myImage.jpg">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:width" content="950">
    <meta prefix="og: http://ogp.me/ns#" property="og:image:height" content="735">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://itzmeanjan.in/">
    <meta property="twitter:title" content="Anjan Roy, Software Engineer, India">
    <meta property="twitter:description" content="My Thoughts, Experiments & Experiences">
    <meta property="twitter:image" content="https://itzmeanjan.in/images/myImage.jpg">
    <meta property="twitter:site" content="@meanjanry">
    <meta name="description" content="My Thoughts, Experiments & Experiences">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Anjan Roy">
    <meta name="keywords"
        content="anjan, roy, itzmeanjan, software, engineer, india, portfolio, skills, projects, thoughts, experiments, experiences">
    <meta name="theme-color" content="darkslategrey">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../styles/index.css">
    <script src="../styles/code.js"></script>
</head>

<body>
    <div id="parentDiv">
        <div id="navBar">
            <nav>
                <a class="navLink" href="/"><big>H</big>ome</a> |
                <a class="navLink" href="https://github.com/itzmeanjan" target="_blank"><big>P</big>rojects</a> |
                <a class="navLink" href="/pages/blog.html"><big>B</big>log</a> |
                <a class="navLink" href="/pages/contact.html"><big>C</big>ontact</a>
            </nav>
        </div>
        <div class="childDiv">
            <article>
                <h1 class="blogHeader">
                    Speaking TCP
                </h1>
                <h3>Created : June 13, 2021</h3>
            </article>
        </div>
        <div class="childDiv">
            <article>
                <p class="blogText">
                    For last few months I've been working at TCP level more often than I generally do.
                    During this period I designed and implemented few systems where multiple participants
                    talk to each other over TCP while following custom application level protocol.
                    I learned the way most of TCP applications written in ( specifically ) Golang
                    can be done in a slight different way so that applications don't end up spawning one go-routine
                    per accepted connection --- resulting into thousands of active go-routines when talking to thousands
                    of concurrent peers. Rather than handling each peer in its own go-routine,
                    proactively attempting to read from socket & spending most of its time in blocked mode; keeping only one socket watcher
                    go-routine which is responsible for informing any READ/ WRITE completion
                    event happening on any of delegated sockets --- consumes way lesser resources.
                    It excels at reducing scope of context switching by bringing possible
                    go-routine count to minimal. As a result of it, Golang scheduler only needs
                    to manage a few go-routines now. Previously scheduler had to orchestrate
                    thousands of go-routines on <b>N</b> system threads. I ran some experiments
                    and result was promising --- TCP servers able to easily handle <b>100k</b> concurrent connections
                    when following second approach.
                </p>
                <p class="blogText">
                    Following 3 different approaches, I develop key-value database where clients can send read/ write requests
                    over TCP. I challenge each implementation with <b>100k</b> concurrent connections
                    and collect statistics of their performance, resource consumption etc. under high load;
                    all running on a consumer-grade GNU/Linux machine in containerised environment i.e. Docker.
                </p>
                <ol>
                    <li>One go-routine per connection</li>
                    <li>One watcher for all sockets</li>
                    <li>N ( >1 ) watchers for all sockets</li>
                </ol>
                <p class="blogText">
                    The application I develop is quite simple but it captures the essence of a TCP application.
                    It's a remote <i>( not necessarily geographically )</i> in-memory KV database, to which clients connect
                    over TCP & maintain that connection throughout their life time. During their life time they do
                    any of two possible operations in a randomised manner.
                </p>
                <ol>
                    <li><b>READ</b> - Attempt to read VALUE associated with supplied KEY</li>
                    <li><b>WRITE</b> - Attempt to associate VALUE with KEY</li>
                </ol>
                <p class="blogText">
                    In both of the cases clients expect to hear back from server. In response frame
                    VALUE associated with KEY is returned. For WRITE request, VALUE in response frame
                    must be equal to what's sent in request frame. On server side all reading/ writing
                    is done in concurrent safe manner --- by acquiring mutex locks. Only for write request
                    r/w lock is held i.e. <i>critical section of code</i>, otherwise normal read-only lock is held --- allowing
                    fulfilment of multiple READ requests concurrently.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-kv-store.jpg">
                <p class="blogText">
                    For performing desired operations, clients send structured data frames
                    over TCP; server extracts that out from socket; performs action as specified
                    in message envelope i.e. opcode field; responds back with response frame.
                    <br>
                    <br>
                    Each message sent over wire is two-parts, where envelope carries operation kind i.e. {READ, WRITE} &
                    how many more bytes server need to be read from stream to construct a structured message. Clients
                    always expect to receive only one kind of frame in response.
                </p>
                <ol>
                    <li>Envelope : 3 bytes</li>
                    <li>Body : N ( < 65536 ) bytes</li>
                </ol>
                <p class="blogText">
                    For a READ frame, sent when client is interested in looking up VALUE associated
                    with KEY, body just holds key, preceded with key length in 1 byte field. Notice, body length
                    field in envelope is 2 bytes, allowing at max 65535 bytes of body, but in body actually
                    256 bytes can be written due to key length field in body being of 1 byte. This is done intensionally
                    for keeping illustration simple.
                    <br>
                    <br>
                    Practically max READ frame size over wire will be
                </p>
                <table class="centeredTable">
                    <tr>
                        <th>Field</th>
                        <th>Max Thoeretical Size ( in bytes )</th>
                        <th>Max Practical Size ( in bytes )</th>
                    </tr>
                    <tr>
                        <td>Envelope</td>
                        <td>3</td>
                        <td>3</td>
                    </tr>
                    <tr>
                        <td>Body</td>
                        <td>65535</td>
                        <td>256</td>
                    </tr>
                    <tr>
                        <td>Total</td>
                        <td>65538</td>
                        <td><b>259</b></td>
                    </tr>
                </table>
                <img class="imgCenter" src="../images/speaking-tcp-read-frame.jpg">
                <p class="blogText">
                    WRITE frame carries little more data, which is sent when client is interested
                    in associating VALUE with some KEY, because it carries both key, value & each of them are preceded
                    with respective length in 1 byte field. Same scene here, practically WRITE frame's body will be at max 512 bytes
                    though it's allowed to be at max 65535 bytes theoretically, as written in body length field in stream.
                    <br>
                    <br>
                    Limits WRITE frame size will be
                </p>
                <table class="centeredTable">
                    <tr>
                        <th>Field</th>
                        <th>Max Thoeretical Size ( in bytes )</th>
                        <th>Max Practical Size ( in bytes )</th>
                    </tr>
                    <tr>
                        <td>Envelope</td>
                        <td>3</td>
                        <td>3</td>
                    </tr>
                    <tr>
                        <td>Body</td>
                        <td>65535</td>
                        <td>512</td>
                    </tr>
                    <tr>
                        <td>Total</td>
                        <td>65538</td>
                        <td><b>515</b></td>
                    </tr>
                </table>
                <img class="imgCenter" src="../images/speaking-tcp-write-frame.jpg">
                <p class="blogText">
                    In response of READ/ WRITE request client expects to receive one RESPONSE frame, where VALUE
                    associated with KEY is encoded, where length of VALUE precedes it, encoded 1 byte --- signaling
                    client how many more bytes to read from stream to construct response. Good thing about 
                    response frame, it doesn't waste any space, just allows sending 255 bytes VALUE at max.
                </p>
                <table class="centeredTable">
                    <tr>
                        <th>Field</th>
                        <th>Max Theoretical Size ( in bytes )</th>
                        <th>Max Practical Size ( in bytes )</th>
                    </tr>
                    <tr>
                        <td>Envelope</td>
                        <td>2</td>
                        <td>2</td>
                    </tr>
                    <tr>
                        <td>Body</td>
                        <td>255</td>
                        <td>255</td>
                    </tr>
                    <tr>
                        <td>Total</td>
                        <td>257</td>
                        <td>257</td>
                    </tr>
                </table>
                <img class="imgCenter" src="../images/speaking-tcp-response-frame.jpg">
                <p class="blogText">
                    Now I'd like to spend some time in specifying how each of 3 approaches work. 
                    For ease of addressing, I'll refer to them from now on as <i>{<b>v1</b> => 1, <b>v2</b> => 2, <b>v3</b> => 3}</i>.
                    <br>
                    <br>
                    Model <b>v1</b> is popular way of writing TCP servers in Go, where one listener go-routine
                    keep listening on a <i>host:port</i>; accepts connection & spawns new go-routine for handling
                    connection throughout its life time. This model respects seperation of concern well & operations
                    happening on socket are easier to reason about due to clean structure. But one thing to notice, each go-routine
                    alive for handling concurrent connections, spends a lot of its time in blocked state --- proactively waiting to read
                    from socket.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-model-v1.jpg">
                <p class="blogText">
                    Model <b>v2</b> is slightly different than <b>v1</b>, where rather than spawning
                    one go-routine per accepted connection, all accepted connections are delegated to
                    one watcher go-routine, which runs one kernel event loop and learns about READ/ WRITE
                    completion events on sockets being watched. Every now and then event loop informs
                    watcher go-routine of READ/ WRITE completion events, providing with opportunity to 
                    take action on accomplished task and schedule next operation on socket asynchronously.
                    <br>
                    <br>
                    This mode of operation has some similarity with <i>libuv</i> --- which powers NodeJS's event loop.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-model-v2.jpg">
                <p class="blogText">
                    I'd call model <b>v3</b> a generic version of model <b>v2</b>, where N-watcher go-routines
                    run N-many kernel event loops and each accepted connection is delegated to one of these
                    available watchers for rest of their life time. Whenever READ/ WRITE completion event ocurrs on
                    some socket, event loop notifies respective watcher go-routine, which invokes <i>handle{READ,WRITE}</i> method
                    to take action on completed event and schedule next operation on socket, to be completed asynchronously.
                    <br>
                    <br>
                    Using this model calls for socket orchestrating technique --- connections are fairly
                    distributed among all available watcher go-routines. Goal of orchestration is not creating hot-spots i.e.
                    some watcher go-routine managing lots of sockets while some has got few. This defeats whole purpose
                    of model <b>v3</b>. One naive orchestration technique will be using modular arithmetic, where
                    M-th accepted connection is delegated to M % N -th watcher go-routine, where M > 0, N > 0, N = #-of watcher go-routines.
                    <br>
                    <br>
                    One problem I see with this scheme is, assuming peer connections are generally long-lived
                    some watcher might end-up managing all those long-lived peers while some other watcher go-routine
                    probably received those sockets which were unfortunately not long-lived, will manage few sockets --- creating
                    imbalance in socket watching delegation i.e. hotspot resulting into bad performance.
                    What I think can be done, rather than blindly orchestrating sockets using naive round-robin technique, 
                    better to keep one feedback loop from watcher go-routines, so that they can inform
                    listener go-routine of their current status i.e. how many delegated sockets are they managing
                    now ?, how many of them are active in terms of READ/ WRITE operation frequency --- rolling average
                    over finite timespan ? etc., allowing listener go-routine to make more informed decision before it
                    delegates accepted connection to some watcher. This brings in management flexibility.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-model-v3.jpg">
                <p class="blogText">
                    It's time to run these models on real mahine and collect statistics. I've prepared parallel
                    benchmarking testcases, where in each round of benchmarking one client connects to TCP server
                    and sends two frames in order. First frame is read request for some KEY, waits for response, consumes
                    it <i>( if some other client has already set VALUE for that KEY )</i>; then it sends write request
                    with a KEY, VALUE pair, waits for response, expecting to see VALUE in response matching what it sent
                    in write request. Each benchmark is performed 8 times, to get average statistics.
                    <br>
                    <br>
                    I do parallel benchmark of model <b>v1</b> on two machines running GNU/Linux & MacOS
                    where for each round takes ~34k ns on GNU/Linux, but it's relatively on higher side
                    when run on MacOS ~45k ns.
                </p>
                <div class="microlight">
                    // v1 on GNU/Linux

                    $ go test -v -run=xxx -bench V1 -count 8
                    goos: linux
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v1
                    cpu: Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
                    BenchmarkServerV1
                    BenchmarkServerV1-4   	   34150	     34866 ns/op	  29.63 MB/s	    3750 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   34316	     34922 ns/op	  29.58 MB/s	    3750 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33979	     35035 ns/op	  29.48 MB/s	    3750 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33944	     35518 ns/op	  29.08 MB/s	    3750 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33795	     35610 ns/op	  29.01 MB/s	    3750 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33493	     35604 ns/op	  29.01 MB/s	    3749 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33490	     35473 ns/op	  29.12 MB/s	    3749 B/op	      52 allocs/op
                    BenchmarkServerV1-4   	   33523	     35338 ns/op	  29.23 MB/s	    3749 B/op	      52 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v1	12.452s
                </div>
                <div class="microlight">
                    // v1 on MacOS

                    $ go test -v -run=xxx -bench V1 -count 8
                    goos: darwin
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v1
                    cpu: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz
                    BenchmarkServerV1
                    BenchmarkServerV1-8   	   27030	     47430 ns/op	  21.78 MB/s	    3751 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   26208	     44778 ns/op	  23.07 MB/s	    3752 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   27098	     43486 ns/op	  23.75 MB/s	    3752 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   27368	     44496 ns/op	  23.22 MB/s	    3753 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   25234	     49744 ns/op	  20.77 MB/s	    3753 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   24418	     49292 ns/op	  20.96 MB/s	    3752 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   24998	     47754 ns/op	  21.63 MB/s	    3751 B/op	      52 allocs/op
                    BenchmarkServerV1-8   	   24969	     47495 ns/op	  21.75 MB/s	    3751 B/op	      52 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v1	14.223s
                </div>
                <p class="blogText">
                    For model <b>v2</b>, MacOS takes lesser time for each round than it took in model <b>v1</b>. But that's not
                    true for GNU/Linux --- rather it almost doubled up.
                </p>
                <div class="microlight">
                    // v2 on GNU/Linux

                    $ go test -v -run=xxx -bench V2 -count 8
                    goos: linux
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v2
                    cpu: Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
                    BenchmarkServerV2
                    BenchmarkServerV2-4   	   19852	     60069 ns/op	  34.39 MB/s	    6113 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   20102	     60362 ns/op	  34.23 MB/s	    6108 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   19983	     59815 ns/op	  34.54 MB/s	    6107 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   20096	     59202 ns/op	  34.90 MB/s	    6107 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   20307	     59099 ns/op	  34.96 MB/s	    6107 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   19944	     60038 ns/op	  34.41 MB/s	    6107 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   20209	     58666 ns/op	  35.22 MB/s	    6107 B/op	      71 allocs/op
                    BenchmarkServerV2-4   	   20170	     58852 ns/op	  35.11 MB/s	    6105 B/op	      71 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v2	14.448s                    
                </div>
                <div class="microlight">
                    // v2 on MacOS

                    $ go test -v -run=xxx -bench V2 -count 8
                    goos: darwin
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v2
                    cpu: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz
                    BenchmarkServerV2
                    BenchmarkServerV2-8   	   35652	     32646 ns/op	  63.29 MB/s	    6191 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   39087	     30548 ns/op	  67.63 MB/s	    6178 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   39044	     30425 ns/op	  67.91 MB/s	    6173 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   39390	     30321 ns/op	  68.14 MB/s	    6175 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   39427	     30540 ns/op	  67.65 MB/s	    6175 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   37016	     34478 ns/op	  59.92 MB/s	    6177 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   35229	     36566 ns/op	  56.50 MB/s	    6186 B/op	      72 allocs/op
                    BenchmarkServerV2-8   	   33456	     35525 ns/op	  58.16 MB/s	    6184 B/op	      72 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v2	12.866s
                </div>
                <p class="blogText">
                    In case of model <b>v3</b>, GNU/Linux and MacOS both of them has kept their trends
                    intact --- for one average benchmark round completion timespan increases, for other
                    it's decreasing, respectively.
                </p>
                <div class="microlight">
                    // v3 on GNU/Linux

                    $ go test -v -run=xxx -bench V3 -count 8
                    goos: linux
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v3
                    cpu: Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
                    BenchmarkServerV3
                    BenchmarkServerV3-4   	   15162	     79368 ns/op	  26.03 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   15229	     78720 ns/op	  26.24 MB/s	    5715 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   14929	     81184 ns/op	  25.45 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   15290	     79059 ns/op	  26.13 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   14955	     79231 ns/op	  26.08 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   15013	     78480 ns/op	  26.33 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   14869	     79838 ns/op	  25.88 MB/s	    5713 B/op	      73 allocs/op
                    BenchmarkServerV3-4   	   15498	     80839 ns/op	  25.56 MB/s	    5713 B/op	      73 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v3	15.678s
                </div>
                <div class="microlight">
                    // v3 on MacOS

                    $ go test -v -run=xxx -bench V3 -count 8
                    goos: darwin
                    goarch: amd64
                    pkg: github.com/itzmeanjan/tseep/v3
                    cpu: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz
                    BenchmarkServerV3
                    BenchmarkServerV3-8   	   41614	     28501 ns/op	  72.49 MB/s	    5715 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   41720	     28395 ns/op	  72.76 MB/s	    5717 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   43344	     27378 ns/op	  75.46 MB/s	    5716 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   43896	     28022 ns/op	  73.73 MB/s	    5712 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   42164	     30386 ns/op	  67.99 MB/s	    5713 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   37576	     32728 ns/op	  63.13 MB/s	    5718 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   37152	     32555 ns/op	  63.46 MB/s	    5714 B/op	      74 allocs/op
                    BenchmarkServerV3-8   	   36784	     31925 ns/op	  64.71 MB/s	    5714 B/op	      74 allocs/op
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v3	12.792s
                </div>
                <p class="blogText">
                    Now I plan to stress test 3 models on both of GNU/Linux & MacOS platform with 8k
                    concurrent connections, where each client connects to TCP server, sends
                    read & write requests in order while waiting for their response
                    in both of the cases.
                    <br>
                    <br>
                    When model <b>v1</b> is stress tested, it completes lot faster on GNU/Linux, given
                    it enjoys benefit of faster CPU.
                </p>
                <div class="microlight">
                    // stress testing v1 on GNU/Linux

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV1_Stress_8k
                    --- PASS: TestServerV1_Stress_8k (0.50s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v1	0.542s



                    // stress testing v1 on MacOS

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV1_Stress_8k
                    --- PASS: TestServerV1_Stress_8k (2.41s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v1	2.714s
                </div>
                <p class="blogText">
                    With 8k concurrent connections model <b>v2</b> takes almost same time to complete
                    on both GNU/Linux & MacOS platform.
                </p>
                <div class="microlight">
                    // stress testing v2 on GNU/Linux

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV2_Stress_8k
                    --- PASS: TestServerV2_Stress_8k (0.60s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v2	0.645s



                    // stress testing v2 on MacOS

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV2_Stress_8k
                    --- PASS: TestServerV2_Stress_8k (2.50s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v2	3.201s
                </div>
                <p class="blogText">
                    Time required for completing stress testing with model <b>v3</b>
                    is almost unchanged for MacOS, but for GNU/Linux it's slightly increasing.
                </p>
                <div class="microlight">
                    // stress testing v3 on GNU/Linux

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV3_Stress_8k
                    --- PASS: TestServerV3_Stress_8k (0.73s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v3	0.757s



                    // stress testing v3 on MacOS

                    $ go test -v -tags stress -run=8k
                    === RUN   TestServerV3_Stress_8k
                    --- PASS: TestServerV3_Stress_8k (2.48s)
                    PASS
                    ok  	github.com/itzmeanjan/tseep/v3	3.095s
                </div>
                <p class="blogText">
                    Go's trace tool is helpful in getting deeper insight into what happened
                    when program was running. So I collect program execution trace when running
                    program testing. These are collected from MacOS machine.
                    <br>
                    <br>
                    While looking for how major go-routines spent their time model <b>v1</b>,
                    I found listener go-routine <i>which accepts connections and spawns new go-routine
                        for handling it</i>, spends a major portion of time in blocked state --- which is understandable
                    because it's waiting for new connection to arrive.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v1-listener-trace.png">
                <p class="blogText">
                    If I now look at how its spawned connection handler go-routine spent its time,
                    I see it has also spent most of its time in waiting for network IO. This also
                    makes sense given the fact, in model <b>v1</b> each connection is handled in its
                    own go-routine, resulting into each of those go-routines proactively waiting
                    to read from socket --- waiting for network IO event.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v1-handle-connection-trace.png">
                <p class="blogText">
                    I look at model <b>v2</b>'s execution trace. It has two major go-routines i.e. {listener, watcher}.
                    Listener does same job in all 3 models --- wait for incoming connection; accept it; prepare connection
                    handling phase <i>( different in each model )</i>; keep looping, which is why network IO based blocking is evident in its trace.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v2-listener-trace.png">
                <p class="blogText">
                    When I look at model <b>v2</b>'s watcher go-routine trace, it doesn't spend any time in waiting
                    for network IO --- it makes a blocking call where it waits for accumulation of a few READ/ WRITE
                    completion event from underlying kernel event loop, as soon as it's done, it starts looping over them and takes necessary actions.
                    This single function is equivalent of what N-many handleConnection go-routine does in model <b>v1</b>.
                    When scheduler wait column is checked in each of these traces, it's well understandable each go-routine
                    spawned needs to be scheduled on underlying OS threads to actually run, and scheduling is not cheap
                    when having 100k go-routines.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v2-watcher-trace.png">
                <p class="blogText">
                    At last I take a look at trace of model <b>v3</b>, where I run 4 watcher go-routines
                    each managing a subset of accepted connections. Listener go-routine's trace is similar
                    to what I found in other models.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v3-listener-trace.png">
                <p class="blogText">
                    Downside of having more go-routines is scheduling cost --- here I run 4 go-routines
                    with 4 different kernel event loop, subset of sockets are delegated to them, resulting
                    into spending more time in scheduler wait stage. Also notice though there're 4 watcher go-routines
                    ready to do their job, not all being used. It's because of the fact during test, 
                    when trace is collected, only one connection request is sent from client side, resulting
                    into only one socket being managed by one of available watcher go-routines.
                </p>
                <img class="imgCenter" src="../images/speaking-tcp-v3-watcher-trace.png">
            </article>
        </div>
    </div>
    <div id="footerDiv">
        <footer>
            <p id="footerText">
                &copy <a href="https://github.com/itzmeanjan/itzmeanjan.github.io" id="footerLink"
                    target="_blank"><big>A</big>njan Roy</a> ( <big>M</big>IT Licensed )
            </p>
        </footer>
    </div>
</body>

</html>
